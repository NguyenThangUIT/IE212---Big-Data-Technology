{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, max as spark_max\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# MongoDB configuration\n",
    "MONGO_URI = \"mongodb://localhost:27017/\"\n",
    "DB_NAME = \"Recommendations\"\n",
    "COLLECTION_NAME = \"Recommendations\"\n",
    "\n",
    "# Kafka & Excel configuration\n",
    "KAFKA_TOPIC = \"SF_weather_data\"\n",
    "KAFKA_BROKER = \"localhost:9092\"\n",
    "EXCEL_FILE = \"khuyen-nghi.xlsx\"\n",
    "\n",
    "# Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SFr-recommend\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema for Kafka data\n",
    "schema = StructType([\n",
    "    StructField(\"temperature\", FloatType(), True),\n",
    "    StructField(\"humidity_air\", FloatType(), True),\n",
    "    StructField(\"pressure\", FloatType(), True),\n",
    "    StructField(\"soil_moisture\", FloatType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB connection\n",
    "mongo_client = MongoClient(MONGO_URI)\n",
    "db = mongo_client[DB_NAME]\n",
    "collection = db[COLLECTION_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from Kafka topic\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "    .option(\"subscribe\", KAFKA_TOPIC) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode Kafka data\n",
    "\n",
    "decoded_df = df.selectExpr(\"CAST(value AS STRING) as json_data\") \\\n",
    "    .select(from_json(col(\"json_data\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm lấy dữ liệu từ Excel\n",
    "def get_recommendations(crop, soil_type, actual_data):\n",
    "    # Đọc Excel\n",
    "    df_excel = pd.read_excel(EXCEL_FILE)\n",
    "\n",
    "    # Lọc dữ liệu theo crop và soil_type\n",
    "    crop_data = df_excel[(df_excel[\"label\"] == crop) & (df_excel[\"Soilcolor\"] == soil_type)]\n",
    "    if crop_data.empty:\n",
    "        return f\"Không tìm thấy dữ liệu cho cây {crop} trên loại đất {soil_type}.\"\n",
    "\n",
    "    crop_data = crop_data.iloc[0]  # Dòng đầu tiên\n",
    "\n",
    "    # Xác định mùa hiện tại\n",
    "    month = datetime.now().month\n",
    "    season = \"W\" if month in [12, 1, 2] else \"Sp\" if month in [3, 4, 5] else \"Su\" if month in [6, 7, 8] else \"Au\"\n",
    "\n",
    "    # Lấy dữ liệu lý tưởng từ Excel\n",
    "    ideal_temp = crop_data[f\"T2M_AVG-{season}\"]\n",
    "    ideal_humidity = crop_data[f\"QV2M-{season}\"]\n",
    "    ideal_precipitation = crop_data[f\"PRECTOTCORR-{season}\"]\n",
    "    ideal_soil_moisture = crop_data[\"GWETTOP\"]\n",
    "    ideal_pressure = crop_data[\"PS\"]\n",
    "    ideal_ph = crop_data[\"Ph\"]\n",
    "\n",
    "    # Tính toán khuyến nghị\n",
    "    temperature_diff = actual_data[\"temperature\"] - ideal_temp\n",
    "    humidity_diff = actual_data[\"humidity_air\"] - ideal_humidity\n",
    "    pressure_diff = actual_data[\"pressure\"] - ideal_pressure\n",
    "    soil_moisture_diff = actual_data[\"soil_moisture\"] - ideal_soil_moisture\n",
    "\n",
    "    recommendations = {\n",
    "        \"temperature\": f\"Điều chỉnh nhiệt độ {'giảm' if temperature_diff > 0 else 'tăng'} {abs(temperature_diff):.2f}°C.\",\n",
    "        \"humidity_air\": f\"Điều chỉnh độ ẩm không khí {'giảm' if humidity_diff > 0 else 'tăng'} {abs(humidity_diff):.2f}%.\",\n",
    "        \"pressure\": f\"Điều chỉnh áp suất {'giảm' if pressure_diff > 0 else 'tăng'} {abs(pressure_diff):.2f} hPa.\",\n",
    "        \"soil_moisture\": f\"Điều chỉnh độ ẩm đất {'giảm' if soil_moisture_diff > 0 else 'tăng'} {abs(soil_moisture_diff):.2f}%.\",\n",
    "        \"irrigation\": f\"Tưới {ideal_precipitation * 10:.2f} m³ nước/ha/ngày.\",\n",
    "        \"soil_pH\": f\"pH đất lý tưởng: {ideal_ph}.\",\n",
    "    }\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xử lý và lưu vào MongoDB\n",
    "def process_and_store(crop, soil_type, actual_data):\n",
    "    recommendations = get_recommendations(crop, soil_type, actual_data)\n",
    "    if isinstance(recommendations, dict):\n",
    "        # Lưu khuyến nghị vào MongoDB\n",
    "        mongo_data = {\n",
    "            \"timestamp\": actual_data[\"timestamp\"],\n",
    "            \"crop\": crop,\n",
    "            \"soil_type\": soil_type,\n",
    "            \"actual_data\": actual_data,\n",
    "            \"recommendations\": recommendations,\n",
    "        }\n",
    "        collection.insert_one(mongo_data)\n",
    "        print(f\"Khuyến nghị đã lưu vào MongoDB: {mongo_data}\")\n",
    "    else:\n",
    "        print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aboyw\\anaconda3\\Lib\\site-packages\\pyspark\\serializers.py\", line 459, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aboyw\\anaconda3\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"c:\\Users\\aboyw\\anaconda3\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 632, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: cannot pickle '_thread.lock' object\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: TypeError: cannot pickle '_thread.lock' object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\aboyw\\anaconda3\\Lib\\site-packages\\pyspark\\serializers.py:459\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cloudpickle\u001b[38;5;241m.\u001b[39mdumps(obj, pickle_protocol)\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPickleError:\n",
      "File \u001b[1;32mc:\\Users\\aboyw\\anaconda3\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[0;32m     70\u001b[0m cp \u001b[38;5;241m=\u001b[39m CloudPickler(\n\u001b[0;32m     71\u001b[0m     file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback\n\u001b[0;32m     72\u001b[0m )\n\u001b[1;32m---> 73\u001b[0m cp\u001b[38;5;241m.\u001b[39mdump(obj)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[1;32mc:\\Users\\aboyw\\anaconda3\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:632\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Pickler\u001b[38;5;241m.\u001b[39mdump(\u001b[38;5;28mself\u001b[39m, obj)\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot pickle '_thread.lock' object",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 40\u001b[0m\n\u001b[0;32m     36\u001b[0m crop \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCây Teff\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     37\u001b[0m soil_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNâu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m query \u001b[38;5;241m=\u001b[39m decoded_df\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;241m.\u001b[39mforeach(process_row) \\\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m     43\u001b[0m query\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[1;32mc:\\Users\\aboyw\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\streaming\\readwriter.py:1384\u001b[0m, in \u001b[0;36mDataStreamWriter.foreach\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m   1382\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_foreach_function(f)\n\u001b[0;32m   1383\u001b[0m serializer \u001b[38;5;241m=\u001b[39m AutoBatchedSerializer(CPickleSerializer())\n\u001b[1;32m-> 1384\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m _wrap_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc, func, serializer, serializer)\n\u001b[0;32m   1385\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1386\u001b[0m jForeachWriter \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39morg\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39msql\u001b[38;5;241m.\u001b[39mexecution\u001b[38;5;241m.\u001b[39mpython\u001b[38;5;241m.\u001b[39mPythonForeachWriter(\n\u001b[0;32m   1388\u001b[0m         wrapped_func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mschema()\n\u001b[0;32m   1389\u001b[0m     )\n\u001b[0;32m   1390\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\aboyw\\anaconda3\\Lib\\site-packages\\pyspark\\rdd.py:5268\u001b[0m, in \u001b[0;36m_wrap_function\u001b[1;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[0;32m   5266\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserializer should not be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5267\u001b[0m command \u001b[38;5;241m=\u001b[39m (func, profiler, deserializer, serializer)\n\u001b[1;32m-> 5268\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m _prepare_for_python_RDD(sc, command)\n\u001b[0;32m   5269\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   5270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSimplePythonFunction(\n\u001b[0;32m   5271\u001b[0m     \u001b[38;5;28mbytearray\u001b[39m(pickled_command),\n\u001b[0;32m   5272\u001b[0m     env,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5277\u001b[0m     sc\u001b[38;5;241m.\u001b[39m_javaAccumulator,\n\u001b[0;32m   5278\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\aboyw\\anaconda3\\Lib\\site-packages\\pyspark\\rdd.py:5251\u001b[0m, in \u001b[0;36m_prepare_for_python_RDD\u001b[1;34m(sc, command)\u001b[0m\n\u001b[0;32m   5248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_for_python_RDD\u001b[39m(sc: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkContext\u001b[39m\u001b[38;5;124m\"\u001b[39m, command: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mbytes\u001b[39m, Any, Any, Any]:\n\u001b[0;32m   5249\u001b[0m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[0;32m   5250\u001b[0m     ser \u001b[38;5;241m=\u001b[39m CloudPickleSerializer()\n\u001b[1;32m-> 5251\u001b[0m     pickled_command \u001b[38;5;241m=\u001b[39m ser\u001b[38;5;241m.\u001b[39mdumps(command)\n\u001b[0;32m   5252\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   5253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) \u001b[38;5;241m>\u001b[39m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mgetBroadcastThreshold(sc\u001b[38;5;241m.\u001b[39m_jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[0;32m   5254\u001b[0m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aboyw\\anaconda3\\Lib\\site-packages\\pyspark\\serializers.py:469\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    467\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, emsg)\n\u001b[0;32m    468\u001b[0m print_exec(sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m--> 469\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPicklingError(msg)\n",
      "\u001b[1;31mPicklingError\u001b[0m: Could not serialize object: TypeError: cannot pickle '_thread.lock' object"
     ]
    }
   ],
   "source": [
    "# Function to process and store recommendations\n",
    "def process_and_store(crop, soil_type, actual_data):\n",
    "    recommendations = get_recommendations(crop, soil_type, actual_data)\n",
    "    if isinstance(recommendations, dict):\n",
    "        # Lưu khuyến nghị vào MongoDB\n",
    "        mongo_data = {\n",
    "            \"timestamp\": actual_data[\"timestamp\"],\n",
    "            \"crop\": crop,\n",
    "            \"soil_type\": soil_type,\n",
    "            \"actual_data\": actual_data,\n",
    "            \"recommendations\": recommendations,\n",
    "        }\n",
    "        collection.insert_one(mongo_data)\n",
    "        print(f\"Khuyến nghị đã lưu vào MongoDB: {mongo_data}\")\n",
    "    else:\n",
    "        print(recommendations)\n",
    "\n",
    "# Main Spark Streaming Process\n",
    "def process_row(row):\n",
    "    actual_data = {\n",
    "        \"temperature\": row.temperature,\n",
    "        \"humidity_air\": row.humidity_air,\n",
    "        \"pressure\": row.pressure,\n",
    "        \"soil_moisture\": row.soil_moisture,\n",
    "        \"timestamp\": row.timestamp,\n",
    "    }\n",
    "    \n",
    "    # Reinitialize MongoDB connection inside the function\n",
    "    mongo_client = MongoClient(MONGO_URI)\n",
    "    db = mongo_client[DB_NAME]\n",
    "    collection = db[COLLECTION_NAME]\n",
    "    \n",
    "    process_and_store(crop, soil_type, actual_data)\n",
    "\n",
    "# Example crop and soil type; Replace these with dynamic input if needed\n",
    "crop = \"Cây Teff\"\n",
    "soil_type = \"Nâu\"\n",
    "\n",
    "query = decoded_df.writeStream \\\n",
    "    .foreach(process_row) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(df, batch_id):\n",
    "    # Convert DataFrame from Spark to Pandas for easier processing\n",
    "    pdf = df.toPandas()\n",
    "    \n",
    "    for index, row in pdf.iterrows():\n",
    "        actual_data = {\n",
    "            \"temperature\": row['temperature'],\n",
    "            \"humidity_air\": row['humidity_air'],\n",
    "            \"pressure\": row['pressure'],\n",
    "            \"soil_moisture\": row['soil_moisture'],\n",
    "            \"timestamp\": row['timestamp'],\n",
    "        }\n",
    "        \n",
    "        # Reinitialize MongoDB connection inside the function\n",
    "        mongo_client = MongoClient(MONGO_URI)\n",
    "        db = mongo_client[DB_NAME]\n",
    "        collection = db[COLLECTION_NAME]\n",
    "        \n",
    "        process_and_store(crop, soil_type, actual_data)\n",
    "\n",
    "# Use foreachBatch instead of foreach\n",
    "query = decoded_df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
